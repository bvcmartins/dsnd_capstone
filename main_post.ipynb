{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This post discuss in detail the project \"Create a Customer Segmentation Report for Arvato Financial Services\", which is one of the Data Scientist's nanodegree capstone options. It is in fact a continuation of a previous project whose solution was posted [here](https://github.com/bvcmartins/dsndProject3). I chose it because of its broad scope which involves a reasonably complex data cleaning procedure, unsupervised learning, analysis of imbalanced data, and prediction using supervised learning tools. In the following I will discuss my solution for it.\n",
    "\n",
    "### The dataset\n",
    "\n",
    "Arvato kindly provided us the following four datasets:\n",
    "\n",
    "1. Azdias: general elements of the german population (891221 entries, 366 features)\n",
    "2. Customers: same features but containing only customers (191652 entries, 369 features)\n",
    "3. Mailout_train: training set containing potential customers chosen to receive mail ads. It also contains information if the ad was responded \n",
    "4. Mailout_test: testing set for the supervised learning model\n",
    "\n",
    "On top of that two other files were provided:\n",
    "1. DIAS Attributes - Values 2017: information about code levels of each attribute\n",
    "2. DIAS Information Levels - Attributes 2017: high-level information about each attribute\n",
    "\n",
    "Most of the features are ordinal and the numbers only represent a label for ranked value levels. Columns marked as float are actually comprised by ints but were only marked that way because they contain NaN, which is itself a float. The latest pandas version allows us to use the type Int64 which supports a integer NaN. \n",
    "\n",
    "There are also 6 features of type object. These are categorical variables, except for EINGEFUEGT_AM, which is datetime.\n",
    "\n",
    "Most of the features contained NaNs. Actually, NaNs comprised almost 10% of all data.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Cleaning this dataset was a relatively complex task. The steps are outlined below:\n",
    "\n",
    "* pre-cleaning\n",
    "* converting missing values to NaN\n",
    "* assessing missing data per feature\n",
    "* assessing missing data per row\n",
    "* converting mixed-type features to ordinal or binary features\n",
    "* one-hot encoding categorical features\n",
    "* standard scaling numerical features\n",
    "\n",
    "#### Pre-cleaning\n",
    "\n",
    "We defined a function to perform general-purpose operations like converting all numeric features to Int64 (support to integer NaN) and make substitutions for some non-standard missing data encodings.\n",
    "\n",
    "#### Converting missing values to NaN\n",
    "\n",
    "The challenge with this step was that the missing data coding was feature-dependent. Most of the missing values were coded as -1 or 0 and some of them were coded (and not listed in file DIAS) as X or XX. The latter were converted to NaNs during pre-cleaning while the former were first converted to a not-used code (-100) to avoid problems with datatype and then to NaNs.\n",
    "\n",
    "#### Assessing missing data per feature\n",
    "\n",
    "After having all missing values converted to NaN, we were able to assess which features had more than 445 000, half of the total number of entries, missing. As shown below, we found 9 features satrisfying this requirement. They correponded to 18% of all missing values and were all dropped.\n",
    "\n",
    "![](./figures/main_40_0.png)\n",
    "\n",
    "#### Assessing missing data per row\n",
    "\n",
    "After analyzing missing data per column, we turned our attention to missing values patterns associated with rows. As shown in the figure below, the distribution of missing data per row is multimodal. We selected the leftmost cluster, with values above 180, for a statistical test.\n",
    "\n",
    "![](./figures/main_47_0.png)\n",
    "\n",
    "We applied the Kolmogorov-Smirnov test to check if the selected rows are overall different than the main body of data. The null hypothesis is that both groups are identical.\n",
    "\n",
    "Because we were executing multiple comparisons, we applied the very strict Bonferroni correction to the p-values.\n",
    "The results showed that the difference between the two groups were significant only for 8.2% of the test features. Note that this number is not a p-value and it should not be compared with the 0.05 significance level. \n",
    "\n",
    "We decided that differences in 8.2% of the test columns was acceptable and we did not drop the rows.\n",
    "\n",
    "#### Data Imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/brunom/projects/dsnd_capstone\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
