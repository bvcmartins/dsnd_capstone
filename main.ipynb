{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunom/.miniconda2/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "import warnings\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import umap\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 500\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('data/azdias.csv')\n",
    "customers = pd.read_csv('data/customers.csv')\n",
    "azdias.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "customers.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_attr = pd.read_excel('data/DIAS Attributes - Values 2017.xlsx', skiprows=[0])\n",
    "dias_attr.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_info = pd.read_excel('data/DIAS Information Levels - Attributes 2017.xlsx', skiprows=[0])\n",
    "dias_info.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "dias_info.replace({'D19_GESAMT_ANZ_12                                    D19_GESAMT_ANZ_24':'D19_GESAMT_ANZ_12-24',\n",
    "                  'D19_BANKEN_ ANZ_12             D19_BANKEN_ ANZ_24':'D19_BANKEN_ ANZ_12-24',\n",
    "                 'D19_TELKO_ ANZ_12                  D19_TELKO_ ANZ_24':'D19_TELKO_ ANZ_12-24',\n",
    "                 'D19_VERSI_ ANZ_12                                       D19_VERSI_ ANZ_24':'D19_VERSI_ ANZ_12-24',\n",
    "                 'D19_VERSAND_ ANZ_12          D19_VERSAND_ ANZ_24':'D19_VERSAND_ ANZ_12-24'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_clean(df):\n",
    "    list_numeric = df.dtypes[(df.dtypes=='float64') | (df.dtypes=='int64') ].index.values.tolist()\n",
    "    df[list_numeric] = df[list_numeric].astype('Int64')\n",
    "    df['OST_WEST_KZ'] = df['OST_WEST_KZ'].map({'W':0, 'O':1}).astype(\"Int64\")\n",
    "    df['CAMEO_INTL_2015'] = df['CAMEO_INTL_2015'].replace({'XX':np.nan})\n",
    "    df['CAMEO_DEUG_2015'] = df['CAMEO_DEUG_2015'].replace({'X':np.nan})\n",
    "    df['CAMEO_DEU_2015'] = df['CAMEO_DEU_2015'].replace({'XX':np.nan})\n",
    "    df['LP_LEBENSPHASE_FEIN'] = df['LP_LEBENSPHASE_FEIN'].replace({0:np.nan}).astype('Int64')\n",
    "    df['LP_LEBENSPHASE_GROB'] = df['LP_LEBENSPHASE_GROB'].replace({0:np.nan}).astype('Int64')\n",
    "    return df.drop('EINGEFUEGT_AM', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pre_clean(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data - convert missing value codes to NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(x, dict_missing):\n",
    "    try:\n",
    "        return dict_missing[x]\n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "def missing_values(df):\n",
    "    missing_df = dias_attr.query('Meaning==\"unknown\"')[['Attribute','Value']].dropna().set_index('Attribute')\n",
    "    missing_df = missing_df['Value'].astype('str').str.split(',',expand=True).T\n",
    "    missing_df = missing_df.applymap(lambda x: int(x) if x!=None else -100)\n",
    "\n",
    "    for i in missing_df.columns.values:\n",
    "        for j in [0,1]:\n",
    "            dict_missing = {missing_df.loc[j,i]:np.nan}\n",
    "            try:\n",
    "                df[i] = df[i].map(lambda x: map_values(x, dict_missing)).astype('Int64')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = missing_values(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess missing data per feature\n",
    "\n",
    "Features with more than 445k entries missing were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drop_list = pd.Series(azdias.isnull().sum()).where(lambda x:  x > 445E3).dropna().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, feature_drop_list):\n",
    "    return df.drop(feature_drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = drop_features(azdias, feature_drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_estimator = KNeighborsRegressor(n_neighbors=5)\n",
    "#impute_estimator = DecisionTreeRegressor(max_features='sqrt', random_state=0)\n",
    "def impute_numeric(df, strategy):\n",
    "    imputer = IterativeImputer(random_state=0, estimator=impute_estimator, verbose=2)\n",
    "    #imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
    "    return imputer.fit(df)\n",
    "    \n",
    "def impute_object(df):\n",
    "    list_mode = df.apply(lambda x: x.mode()[0]).values.tolist()\n",
    "    list_columns = df.columns.values.tolist()\n",
    "    dict_mode = {i:j for i,j, in zip(list_columns, list_mode)}\n",
    "    return df.fillna(value=dict_mode)\n",
    "    #imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    #return imputer.fit(df)\n",
    "    \n",
    "def impute_calc(df):\n",
    "    list_numeric = df.dtypes[(df.dtypes=='int64') | \n",
    "                             (df.dtypes=='Int64') | \n",
    "                             (df.dtypes=='float64')].index[1:].tolist()\n",
    "\n",
    "    imputer_num = impute_numeric(df[list_numeric].astype('Int64'),'median')\n",
    "    df[list_numeric] = imputer_num.transform(df[list_numeric].astype('Int64')).astype('int64')\n",
    "    \n",
    "    list_objects = df.dtypes[(df.dtypes=='object')].index.tolist()\n",
    "    df[list_objects] = impute_object(df[list_objects])\n",
    "    \n",
    "    return df, imputer_num, list_numeric, list_objects\n",
    "\n",
    "def impute_transform(df):\n",
    "    df[list_numeric] = imputer_num.transform(df[list_numeric].astype('Int64')).astype('int64')\n",
    "    df[list_objects] = impute_object(df[list_objects])\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (891221, 351)\n"
     ]
    }
   ],
   "source": [
    "azdias2, imputer_num, imputer_obj, list_numeric, list_objects = impute_calc(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = azdias2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-encode mixed features\n",
    "\n",
    "After cleaning the data of all NaNs, the next step was to re-encode the variables with mixed fetures. The variables were:\n",
    "\n",
    "* PRAEGENDE_JUGENDJAHRE\n",
    "* CAMEO_INTL_2015\n",
    "* LP_LEBENSPHASE_FEIN\n",
    "* LP_LEBENSPHASE_GROB\n",
    "\n",
    "Variable  PLZ8_BAUMAX could also have been reencoded but the explanatory gains were not clear. The description of the new variables and their levels is presented on the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRAEGENDE_JUGENDJAHRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pragende_jugendjahre(df):\n",
    "    pji_dict = {1:1, 2:1, 3:2, 4:2, 5:3, 6:3, 7:3, 8:4, 9:4, 10:5, 11:5, 12:5, 13:5, 14:6, 15:6}\n",
    "    pjt_dict = {1:0, 2:1, 3:0, 4:1, 5:0, 6:1, 7:1, 8:0, 9:1, 10:0, 11:1, 12:0, 13:1, 14:0, 15:1}\n",
    "    df['PRAEGENDE_JUGENDJAHRE_intervall'] = \\\n",
    "    df['PRAEGENDE_JUGENDJAHRE'].apply(lambda x: pji_dict[int(x)])\n",
    "    df['PRAEGENDE_JUGENDJAHRE_trend'] = \\\n",
    "    df['PRAEGENDE_JUGENDJAHRE'].apply(lambda x: pjt_dict[int(x)])\n",
    "# drop original attribute from dataset\n",
    "    return df.drop('PRAEGENDE_JUGENDJAHRE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pragende_jugendjahre(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-type variable CAMEO_INTL_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cameo_intl_2015(df):\n",
    "    df['CAMEO_INTL_2015'] = df['CAMEO_INTL_2015'].astype('int64')\n",
    "    cir_dict = {11:1, 12:1, 13:1, 14:1, 15:1, 21:2, 22:2, 23:2, 24:2, 25:2, 31:3, 32:3, 33:3, 34:3, \n",
    "            35:3, 41:4, 42:4, 43:4, 44:4, 45:4, 51:5, 52:5, 53:5, 54:5, 55:5}\n",
    "    cil_dict = {11:1, 12:2, 13:3, 14:4, 15:5, 21:1, 22:2, 23:3, 24:4, 25:5, 31:1, 32:2, 33:3, 34:4, \n",
    "            35:5, 41:1, 42:2, 43:3, 44:4, 45:5, 51:1, 52:2, 53:3, 54:4, 55:5}\n",
    "    df['CAMEO_INTL_2015_reichtum'] = df['CAMEO_INTL_2015'].map(cir_dict).astype('int64')\n",
    "    df['CAMEO_INTL_2015_leben'] = df['CAMEO_INTL_2015'].map(cil_dict).astype('int64')\n",
    "    return df.drop('CAMEO_INTL_2015', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = cameo_intl_2015(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-type variable LP_LEBENSPHASE_FEIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_lebensphase_fein(df):\n",
    "    df['LP_LEBENSPHASE_FEIN'] = df['LP_LEBENSPHASE_FEIN'].astype('int64')\n",
    "    llfa_dict={1:1, 2:2, 3:1, 4:2, 5:4, 6:3, 7:4, 8:3, 9:2, 10:6, 11:4, 12:3, 13:5, 14:1, 15:5, \n",
    "           16:5, 17:2, 18:1, 19:5, 20:5, 21:2, 22:2, 23:2, 24:2, 25:2, 26:2, 27:2, 28:2, 29:1, 30:1, \n",
    "           31:5, 32:5, 33:1, 34:1, 35:1, 36:5, 37:4, 38:3, 39:2, 40:3}\n",
    "\n",
    "    llfv_dict={1:1, 2:1, 3:2, 4:2, 5:1, 6:1, 7:2, 8:2, 9:3, 10:3, 11:4, 12:4, 13:5, 14:2, 15:1, \n",
    "           16:2, 17:3, 18:6, 19:4, 20:5, 21:1, 22:2, 23:5, 24:1, 25:2, 26:3, 27:4, 28:5, 29:1, 30:2, \n",
    "           31:1, 32:2, 33:3, 34:4, 35:5, 36:3, 37:4, 38:4, 39:5, 40:5}\n",
    "\n",
    "    llff_dict={1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, 13:1, 14:5, 15:5, \n",
    "           16:5, 17:5, 18:5, 19:5, 20:5, 21:2, 22:2, 23:2, 24:4, 25:4, 26:4, 27:4, 28:4, 29:3, 30:3, \n",
    "           31:3, 32:3, 33:3, 34:3, 35:3, 36:3, 37:3, 38:3, 39:3, 40:3}\n",
    "\n",
    "    # transformation of LP_LEBENSPHASE_FEIN\n",
    "    df['LP_LEBENSPHASE_FEIN_alter'] = df['LP_LEBENSPHASE_FEIN'].map(llfa_dict).astype('int64')\n",
    "    df['LP_LEBENSPHASE_FEIN_verdiener'] = df['LP_LEBENSPHASE_FEIN'].map(llfv_dict).astype('int64')\n",
    "    df['LP_LEBENSPHASE_FEIN_familie'] = df['LP_LEBENSPHASE_FEIN'].map(llff_dict).astype('int64')\n",
    "    # drop original attribute from dataset\n",
    "    return df.drop('LP_LEBENSPHASE_FEIN', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = lp_lebensphase_fein(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-type variable LP_LEBENSPHASE_GROB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_lebensphase_grob(df):\n",
    "    df['LP_LEBENSPHASE_GROB'] = df['LP_LEBENSPHASE_GROB'].astype('int64')\n",
    "    llga_dict={1:1, 2:3, 3:2, 4:2, 5:2, 6:2, 7:2, 8:2, 9:1, 10:3, 11:1, 12:3}\n",
    "    llgv_dict={1:0, 2:0, 3:1, 4:0, 5:1, 6:0, 7:0, 8:1, 9:0, 10:0, 11:1, 12:1}\n",
    "    llgf_dict={1:1, 2:1, 3:1, 4:2, 5:2, 6:5, 7:3, 8:3, 9:4, 10:4, 11:4, 12:4}\n",
    "    \n",
    "    df['LP_LEBENSPHASE_GROB_alter'] = df['LP_LEBENSPHASE_GROB'].map(llga_dict).astype('int64')\n",
    "    df['LP_LEBENSPHASE_GROB_verdiener'] = df['LP_LEBENSPHASE_GROB'].map(llgv_dict).astype('int64')\n",
    "    df['LP_LEBENSPHASE_GROB_familie'] = df['LP_LEBENSPHASE_GROB'].map(llgf_dict).astype('int64')\n",
    "    # drop original attribute from dataset\n",
    "    return df.drop('LP_LEBENSPHASE_GROB', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = lp_lebensphase_grob(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lists(df):\n",
    "    list_all = df.columns.tolist()[1:]\n",
    "    list_onehot = dias_info['Attribute'].str.extract(r'([0-9A-Z_]*TYP)', expand=True).\\\n",
    "    dropna().\\\n",
    "    stack().\\\n",
    "    values.\\\n",
    "    tolist()\n",
    "    list_onehot = list(set(list_onehot).intersection(set(list_all)))\n",
    "    list_binary = [column for column in df.columns.tolist() if df[column].value_counts().shape[0]==2]\n",
    "    #list_onehot = set(list_onehot).difference(set(list_binary))\n",
    "    list_scale = list(set(list_all).difference(set(list_binary)).difference(set(list_onehot)))\n",
    "    \n",
    "    # specific corrections\n",
    "    list_scale.remove('D19_LETZTER_KAUF_BRANCHE')\n",
    "    list_onehot.append('D19_LETZTER_KAUF_BRANCHE')\n",
    "    list_scale.remove('CAMEO_DEUG_2015')\n",
    "    list_onehot.append('CAMEO_DEUG_2015')\n",
    "    list_scale.remove('CAMEO_DEU_2015')\n",
    "    list_onehot.append('CAMEO_DEU_2015')\n",
    "    \n",
    "    return list_onehot, list_binary, list_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_onehot, list_binary, list_scale = make_lists(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_types(df):\n",
    "    df['CAMEO_DEUG_2015'] = df['CAMEO_DEUG_2015'].astype('int')\n",
    "    return df.drop('LNR', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = adjust_types(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder()\n",
    "onehot.fit(azdias[list_onehot])\n",
    "df_onehot = pd.DataFrame(data = onehot.transform(azdias[list_onehot]).todense(), \n",
    "                         columns=onehot.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_onehot = pd.concat([azdias.drop(list_onehot, axis=1), df_onehot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "#scaler = StandardScaler()\n",
    "azdias_onehot[list_scale] = scaler.fit_transform(azdias_onehot[list_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_onehot.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    return df[(np.abs(stats.zscore(df[list_scale])) < 4).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias2 = remove_outliers(azdias_onehot.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_components(df, minRange, maxRange):\n",
    "    evr = {}\n",
    "    for i in range(minRange, maxRange):\n",
    "        pca_model = PCA(n_components=i)\n",
    "        X_pca = pca_model.fit_transform(df)\n",
    "        evr[i] = pca_model.explained_variance_ratio_.sum()\n",
    "    return evr\n",
    "\n",
    "def generate_pca(df, n_components):\n",
    "    '''\n",
    "    Generates PCA model\n",
    "    \n",
    "    INPUT: df - scaled dataframe\n",
    "           n_components - number of components for the model\n",
    "           \n",
    "    OUTPUT: pca_model - PCA object\n",
    "            var_pca - dataframe with components and explained variances\n",
    "            X_pca - numpy array with transformed data\n",
    "    '''\n",
    "    pca_model = PCA(n_components)\n",
    "    X_pca = pca_model.fit_transform(df)\n",
    "    components = pd.DataFrame(np.round(pca_model.components_, 4), columns = df.keys())\n",
    "    ratios = pca_model.explained_variance_ratio_.reshape(len(pca_model.components_),1)\n",
    "    dimensions = ['Dim {}'.format(i) for i in range(len(pca_model.components_))]\n",
    "    components.index = dimensions\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios,4), columns=['Explained_Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "    var_pca = pd.concat([variance_ratios, components], axis=1)\n",
    "    \n",
    "    return pca_model, var_pca, X_pca\n",
    "\n",
    "def scree_plot_pca(pca):\n",
    "    '''\n",
    "    Creates a scree plot associated with the principal components \n",
    "    \n",
    "    INPUT: pca - the result of instantiating of PCA in scikit learn\n",
    "            \n",
    "    OUTPUT: None\n",
    "    '''\n",
    "    \n",
    "    num_comp = len(pca.explained_variance_ratio_)\n",
    "    idx = np.arange(num_comp)\n",
    "    vals = pca.explained_variance_ratio_\n",
    "    \n",
    "    plt.figure(figsize=(16,6))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.bar(idx, vals*10)\n",
    "    ax.plot(idx, np.cumsum(vals),'r--')\n",
    "    \n",
    "    #for i in range(num_comp):\n",
    "\n",
    "        #ax.annotate(r\"%s\" % ((str(vals[i]*100)[:4])), \n",
    "        #            (idx[i], vals[i]*10), \n",
    "        #            va=\"bottom\", \n",
    "        #            ha=\"center\", \n",
    "        #            fontsize=8)\n",
    "        \n",
    "    #ax.xaxis.set_tick_params(width=0)\n",
    "    #ax.yaxis.set_tick_params(width=0)\n",
    "    \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained\")\n",
    "    \n",
    "    plt.title(\"Explained Variance per Principal Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evr = pca_components(azdias_onehot, 100, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_dim = [key for key, value in evr.items() if value>=0.8][0]\n",
    "#n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 107\n",
    "list_pca = azdias2.columns.values.tolist()\n",
    "pca_model, var_pca, X_pca = generate_pca(azdias2, n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scree_plot_pca(pca_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "Our first attempt with clustering was with k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_centroid(X_pca, k_model):\n",
    "    '''\n",
    "    Calculates the average distance between points in a certain cluster\n",
    "    and the cluster centroid.\n",
    "    \n",
    "    INPUT: X_pca - transformed PCA dimension\n",
    "           k_model - instantiated k-means model\n",
    "           \n",
    "    OUTPUT: scalar mean distance\n",
    "    '''\n",
    "    dist = []\n",
    "    for i, c in enumerate(k_model.cluster_centers_):\n",
    "        a = np.array([np.sqrt(np.dot((x - c),(x - c))) for x in X_pca[k_model.labels_ == i]])\n",
    "        #print(a.shape)\n",
    "        #print(a.sum())\n",
    "        #print((k_model.labels_==i).sum())\n",
    "        dist.append(a.sum() / (k_model.labels_ == i).sum())\n",
    "    \n",
    "    return np.array(dist).mean()\n",
    "\n",
    "def scree_plot_kmeans(X_pca):\n",
    "    '''\n",
    "    Generates scree plot for k-means model\n",
    "    with 1 to 20 components\n",
    "    \n",
    "    INPUT: X_pca - numpy array with transformed data\n",
    "           \n",
    "    OUTPUT: scalar mean distance\n",
    "    '''\n",
    "    \n",
    "    k_score = []\n",
    "    k_dist = []\n",
    "    k_step = []\n",
    "    for k in range(5,25):\n",
    "        k_model = KMeans(n_clusters = k, random_state=34).fit(X_pca)\n",
    "        a = k_model.score(X_pca)\n",
    "        k_score.append(a)\n",
    "        b = dist_centroid(X_pca, k_model)\n",
    "        print('clusters: {}, score: {}, dist: {}'.format(k, a, b))\n",
    "        k_dist.append(b)\n",
    "        k_step.append(k)\n",
    "    return k_step, k_dist, k_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_step, k_dist, k_score = scree_plot_kmeans(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_clusters(method):\n",
    "    model= method.fit(X_pca)\n",
    "    df = pd.DataFrame(X_pca)\n",
    "    df['clusters'] = model.labels_\n",
    "    \n",
    "    return df, model\n",
    "\n",
    "def plot_clusters(df):\n",
    "    palette = sns.color_palette(\"Set2\",11).as_hex()\n",
    "    colors = []\n",
    "    colors.extend([palette[10], palette[9], palette[8], palette[7], palette[6]])\n",
    "\n",
    "    fig, axis = plt.subplots(1, figsize=(8,6))\n",
    "\n",
    "    sns.scatterplot(x=0, y=1, data=df, ax=axis, hue='clusters')\n",
    "\n",
    "def plot_pops(df):\n",
    "    df_ = df.groupby('clusters').agg({'clusters':'size'}).rename(columns={'clusters':'size'}).reset_index()\n",
    "    sns.barplot(x = 'clusters', y = 'size', data=df_, color='gray')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 12\n",
    "kmeans_df, kmeans =  calc_clusters(KMeans(n_clusters = n_clusters, random_state=34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pops(kmeans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data processing to customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer(df):\n",
    "    df_extra = df[['PRODUCT_GROUP','CUSTOMER_GROUP','ONLINE_PURCHASE']]\n",
    "    return df.drop(['PRODUCT_GROUP','CUSTOMER_GROUP','ONLINE_PURCHASE'], axis=1), df_extra\n",
    "\n",
    "def pipeline(df):\n",
    "    df = pre_clean(df)\n",
    "    df = missing_values(df)\n",
    "    df = drop_features(df, feature_drop_list)\n",
    "    #df = drop_rows(df)\n",
    "    df = impute_transform(df)\n",
    "    df = pragende_jugendjahre(df)\n",
    "    df = cameo_intl_2015(df)\n",
    "    df = lp_lebensphase_fein(df)\n",
    "    df = lp_lebensphase_grob(df)\n",
    "    df = adjust_types(df)\n",
    "    #df, df_extra = clean_customer(df)\n",
    "    df_onehot = pd.DataFrame(data = onehot.transform(df[list_onehot]).todense(), \n",
    "                         columns=onehot.get_feature_names())\n",
    "    df = pd.concat([df.drop(list_onehot, axis=1), df_onehot], axis=1)\n",
    "    df[list_scale] = scaler.transform(df[list_scale])\n",
    "    df = remove_outliers(df)\n",
    "    X_pca = pca_model.transform(df[list_pca])\n",
    "    kmeans_pipe = pd.DataFrame(X_pca)\n",
    "    kmeans_pipe['clusters'] = kmeans.predict(X_pca)\n",
    "    \n",
    "    return df, kmeans_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_mod = customers.drop(['PRODUCT_GROUP','CUSTOMER_GROUP','ONLINE_PURCHASE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers, kmeans_customers = pipeline(customers_mod.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('data/mailout_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailout_train, kmeans_train = pipeline(mailout_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailout_train['clusters'] = kmeans_train['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_train['RESPONSE'] = df_mailout_train['RESPONSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA mailout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try tsne\n",
    "df_tsne = pd.DataFrame(TSNE(n_components=2, perplexity=5).fit_transform(kmeans_train.iloc[:,:-2]))\n",
    "df_tsne['response'] = kmeans_train.RESPONSE\n",
    "sns.scatterplot(x=0, y=1, data=df_tsne, hue='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(TSNE(n_components=2, perplexity=10).fit_transform(kmeans_train.iloc[:,:-2]))\n",
    "df_tsne['response'] = kmeans_train.RESPONSE\n",
    "sns.scatterplot(x=0, y=1, data=df_tsne, hue='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=0, y=1, data=df_tsne.query('response==1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(TSNE(n_components=2, perplexity=50).fit_transform(kmeans_train.iloc[:,:-2]))\n",
    "df_tsne['response'] = kmeans_train.RESPONSE\n",
    "sns.scatterplot(x=0, y=1, data=df_tsne, hue='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=0, y=1, data=df_tsne.query('response==1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = df_mailout_train.iloc[:,:-1]\n",
    "Xt.drop('RESPONSE', axis=1, inplace=True)\n",
    "\n",
    "dummies = pd.get_dummies(kmeans_train.clusters, prefix='cluster')\n",
    "Xt = pd.concat([Xt,dummies], axis=1)\n",
    "\n",
    "yt = kmeans_train.RESPONSE\n",
    "X_traint, X_testt, y_traint, y_testt = train_test_split(Xt, yt, test_size=0.33, shuffle=True, random_state=34)\n",
    "\n",
    "model3j = XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
    "              nthread=None, objective='binary:logistic', random_state=34,\n",
    "              reg_alpha=0, reg_lambda=1, sample_pos_weight=80,\n",
    "              scale_pos_weight=1, seed=34, silent=None, subsample=0.8,\n",
    "              verbosity=1)\n",
    "model3j = skf_noSMOTE(X_traint, y_traint.values, model3j)\n",
    "pred_model(model3j, X_testt, y_testt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=3, min_samples=2).fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=0, y=1, data=kmeans_train, hue='RESPONSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_train.clusters.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=0, y=1, data=kmeans_train, hue='clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(kmeans_train.clusters, prefix='cluster')\n",
    "kmeans_train_extended = pd.concat([kmeans_train.iloc[:,:-2],dummies], axis=1)\n",
    "kmeans_train_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailout_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mailout_train_extended = pd.concat([df_mailout_train.iloc[:,:-1],dummies], axis=1)\n",
    "y2 = df_mailout_train.RESPONSE\n",
    "X2 = df_mailout_train.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kmeans_train.RESPONSE\n",
    "X = kmeans_train_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.reset_index(drop=True)\n",
    "y_train2 = y_train.reset_index(drop=True)\n",
    "X_test2 = X_test.reset_index(drop=True)\n",
    "y_test2 = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skf_noSMOTE(X, y, model): \n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=34)\n",
    "    for train_indices, test_indices in skf.split(X, y):\n",
    "    \n",
    "        X_train_skf = X.iloc[train_indices,:]\n",
    "        y_train_skf = y[train_indices]\n",
    "    \n",
    "        X_test_skf = X.iloc[test_indices,:]\n",
    "        y_test_skf = y[test_indices]\n",
    "        \n",
    "        model.fit(X_train_skf, y_train_skf)            \n",
    "        labels.extend(y_test_skf)\n",
    "        preds.extend(model.predict(X_test_skf))\n",
    "    \n",
    "    print('accuracy :', accuracy_score(labels, preds)) \n",
    "    print('F1 :',f1_score(labels, preds))\n",
    "    print('precision :', precision_score(labels, preds))\n",
    "    print('recall :', recall_score(labels, preds))\n",
    "    print('auc :', roc_auc_score(labels, preds))\n",
    "    display(confusion_matrix(labels, preds))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_model(model, X_test, y_test):\n",
    "    y_preds = model.predict(X_test)\n",
    "    print('accuracy :', accuracy_score(y_test, y_preds)) \n",
    "    print('F1 :',f1_score(y_test, y_preds))\n",
    "    print('precision :', precision_score(y_test, y_preds))\n",
    "    print('recall :', recall_score(y_test, y_preds))\n",
    "    print('auc :', roc_auc_score(y_test, y_preds))\n",
    "    y_preds_proba = model.predict_proba(X_test)\n",
    "    print('auc :', roc_auc_score(y_test, y_preds_proba[:,1]))\n",
    "    display(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = df_mailout_train.iloc[:,:-1]\n",
    "Xt.drop('RESPONSE', axis=1, inplace=True)\n",
    "yt = kmeans_train.RESPONSE\n",
    "X_traint, X_testt, y_traint, y_testt = train_test_split(Xt, yt, test_size=0.33, shuffle=True, random_state=34)\n",
    "\n",
    "model1 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
    "              nthread=None, objective='binary:logistic', random_state=34,\n",
    "              reg_alpha=0, reg_lambda=1, sample_pos_weight=80,\n",
    "              scale_pos_weight=1, seed=34, silent=None, subsample=0.8,\n",
    "              verbosity=1)\n",
    "model1 = skf_noSMOTE(X_traint, y_traint.values, model1)\n",
    "pred_model(model1, X_testt, y_testt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = df_mailout_train.iloc[:,:-1]\n",
    "Xt.drop('RESPONSE', axis=1, inplace=True)\n",
    "yt = kmeans_train.RESPONSE\n",
    "X_traint, X_testt, y_traint, y_testt = train_test_split(Xt, yt, test_size=0.33, shuffle=True, random_state=34)\n",
    "input_size = X_traint.shape[1]\n",
    "num_labels = 2\n",
    "batch_size = 128\n",
    "class_weight = {0: 1, 1: 80}\n",
    "\n",
    "y_train3 = to_categorical(y_traint.values)\n",
    "y_test3 = to_categorical(y_testt.values)\n",
    "X_train3 = np.reshape(X_traint.values, [-1, input_size])\n",
    "X_test3 = np.reshape(X_testt.values, [-1, input_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.6\n",
    "hidden_units = 256\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(hidden_units, input_dim=input_size))\n",
    "mlp.add(Activation('relu'))\n",
    "mlp.add(Dropout(dropout))\n",
    "mlp.add(Dense(hidden_units))\n",
    "mlp.add(Activation('relu'))\n",
    "mlp.add(Dropout(dropout))\n",
    "mlp.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector\n",
    "mlp.add(Activation('softmax'))\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "adam = Adam(lr=0.001)\n",
    "sgd = SGD(lr=0.001, momentum=0., decay=0., nesterov=True)\n",
    "mlp.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "mlp.fit(X_train3, y_train3, epochs=epochs, batch_size=batch_size, class_weight = class_weight, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AdaBoostClassifier(random_state=34)\n",
    "model1 = skf_noSMOTE(X_train2, y_train2, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier(random_state=34)\n",
    "model2 = skf_noSMOTE(X_train2, y_train2, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3 = XGBClassifier(booster='dart', max_depth=5, n_estimators=100, n_jobs=-1, random_state=34)\n",
    "model3 = XGBClassifier(random_state=34)\n",
    "model3 = skf_noSMOTE(X_train2, y_train2, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skf_SMOTE(X, y, model):\n",
    "    \n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=34)\n",
    "    for train_indices, test_indices in skf.split(X, y):\n",
    "\n",
    "        X_train_skf = X.iloc[train_indices,:]\n",
    "        y_train_skf = y[train_indices]\n",
    "    \n",
    "        X_test_skf = X.iloc[test_indices,:]\n",
    "        y_test_skf = y[test_indices]\n",
    "        \n",
    "        sm = SMOTE(random_state=34)\n",
    "        X_res, y_res = sm.fit_resample(X_train_skf, y_train_skf)\n",
    "        \n",
    "        model.fit(X_res, y_res)\n",
    "                \n",
    "        labels.extend(y_res)\n",
    "        preds.extend(model.predict(X_res))\n",
    "    \n",
    "    print('accuracy :', accuracy_score(labels, preds)) \n",
    "    print('F1 :',f1_score(labels, preds))\n",
    "    print('precision :', precision_score(labels, preds))\n",
    "    print('recall :', recall_score(labels, preds))\n",
    "    print('auc :', roc_auc_score(labels, preds))\n",
    "    display(confusion_matrix(labels, preds))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X2, y2, test_size=0.33, random_state=34)\n",
    "model12 = AdaBoostClassifier(random_state=34, learning_rate=1., n_estimators=200)\n",
    "model12 = skf_noSMOTE(X_train4, y_train4, model12)\n",
    "pred_model(model12, X_test4.values, y_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AdaBoostClassifier(random_state=34, n_estimators=1000, learning_rate=0.1)\n",
    "model4 = AdaBoostClassifier(random_state=34, learning_rate=0.1, n_estimators=100)\n",
    "model4 = skf_SMOTE(X_train2, y_train2, model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = RandomForestClassifier(random_state=34)\n",
    "model5 = skf_SMOTE(X_train2, y_train2, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = XGBClassifier(random_state=34)\n",
    "model6 = skf_SMOTE(X_train2, y_train2, model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = RandomForestClassifier(random_state=34, n_estimators = 100)\n",
    "model7 = skf_SMOTE(X_train2, y_train2, model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "model8 = XGBClassifier(booster='dart', max_depth=6, learning_rate=0.1, n_estimators=200, n_jobs=-1, random_state=34)\n",
    "model8 = skf_SMOTE(X_train2, y_train2, model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_model(model, X_test, y_test):\n",
    "    y_preds = model.predict(X_test)\n",
    "    print('accuracy :', accuracy_score(y_test, y_preds)) \n",
    "    print('F1 :',f1_score(y_test, y_preds))\n",
    "    print('precision :', precision_score(y_test, y_preds))\n",
    "    print('recall :', recall_score(y_test, y_preds))\n",
    "    print('auc :', roc_auc_score(y_test, y_preds))\n",
    "    y_preds_proba = model.predict_proba(X_test)\n",
    "    print('auc :', roc_auc_score(y_test, y_preds_proba[:,1]))\n",
    "    display(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model1, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model3, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model4, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model5, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model6, X_test2.values, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model7, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model(model8, X_test2.values, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = AdaBoostClassifier(random_state=34, learning_rate=0.05, n_estimators=100)\n",
    "model9 = skf_SMOTE(X_train2, y_train2, model9)\n",
    "pred_model(model9, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=200, n_jobs=-1, random_state=34)\n",
    "model10 = skf_SMOTE(X_train2, y_train2, model10)\n",
    "pred_model(model10, X_test2.values, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = AdaBoostClassifier(random_state=34, learning_rate=1., n_estimators=200)\n",
    "model11 = skf_SMOTE(X_train2, y_train2, model11)\n",
    "pred_model(model11, X_test2.values, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11b = AdaBoostClassifier(random_state=34, learning_rate=1., n_estimators=200)\n",
    "model11b = skf_SMOTE(X_train2b, y_train2b, model11b)\n",
    "pred_model(model11b, X_test2.values, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = XGBClassifier(learning_rate=0.01, n_estimators=1000, n_jobs=-1, random_state=34)\n",
    "model12 = skf_SMOTE(X_train2, y_train2, model12)\n",
    "pred_model(model12, X_test2.values, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = df_mailout_train.iloc[:,:-1]\n",
    "Xt.drop('RESPONSE', axis=1, inplace=True)\n",
    "yt = kmeans_train.RESPONSE\n",
    "X_traint, X_testt, y_traint, y_testt = train_test_split(Xt, yt, test_size=0.33, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_traint.shape[1]\n",
    "num_labels = 2\n",
    "batch_size = 128\n",
    "class_weight = {0: 1, 1: 80}\n",
    "#sm = SMOTE(random_state=34)\n",
    "#X_res, y_res = sm.fit_resample(X_traint, y_traint)\n",
    "y_train3 = to_categorical(y_traint.values)\n",
    "y_test3 = to_categorical(y_testt.values)\n",
    "X_train3 = np.reshape(X_traint.values, [-1, input_size])\n",
    "X_test3 = np.reshape(X_testt.values, [-1, input_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.6\n",
    "hidden_units = 256\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "adam = Adam(lr=0.001)\n",
    "sgd = SGD(lr=0.001, momentum=0., decay=0., nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train3, y_train3, epochs=epochs, batch_size=batch_size, class_weight = class_weight, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(X, y, model):\n",
    "    \n",
    "    y_preds = model.predict(X)\n",
    "\n",
    "    df_pred = pd.DataFrame(y[:,0].astype('int64'), columns=['true'])\n",
    "    df_pred['true'] = df_pred['true'].apply(lambda x: 0 if x==1 else 1)\n",
    "    df_pred['preds'] = np.array(y_preds[:,0])\n",
    "    df_pred['preds'] = df_pred['preds'].apply(lambda x: 1 if x < 0.5 else 0)\n",
    "    df_pred['preds_proba'] = np.array(y_preds[:,0])\n",
    "\n",
    "    class0 = df_pred[(df_pred['true']==1) & (df_pred['preds']==1)].shape[0]\n",
    "    #print(class0)\n",
    "    all_entries = df_pred.shape[0]\n",
    "    print(accuracy_score(df_pred['true'], df_pred['preds']))\n",
    "    print(roc_auc_score(df_pred['true'], df_pred['preds']))\n",
    "    print(roc_auc_score(df_pred['true'], df_pred['preds_proba']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_nn(X_test3, y_test3, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df_mailout_train.iloc[:,:-1]\n",
    "y2 = df_mailout_train.RESPONSE\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X2, y2, test_size=0.2, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = XGBClassifier(learning_rate=0.01, n_estimators=1000, n_jobs=-1, random_state=34)\n",
    "model12 = skf_SMOTE(X_train4, y_train4, model12)\n",
    "pred_model(model12, X_test4.values, y_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer  this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#, kmeans_pipe = pipeline(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer = (customer.pipe(pre_clean)\n",
    "#      .pipe(missing_values)\n",
    "#      .pipe(drop_features, arg1=445E3)\n",
    "#      .pipe(drop_rows).pipe(impute_calc)\n",
    "#      .pipe(pragende_jugendjahre)\n",
    "#      .pipe(cameo_intl_2015)\n",
    "#      .pipe(lp_lebensphase_fein)\n",
    "#      .pipe(lp_lebensphase_grob)\n",
    "#      .pipe(adjust_types))\n",
    "#\n",
    "#def dummies_scale(df):\n",
    "#    df_onehot = pd.DataFrame(data = onehot.transform(df[list_onehot]).todense(), \n",
    "#                         columns=onehot.get_feature_names())\n",
    "#    df = pd.concat([df.drop(list_onehot, axis=1), df_onehot], axis=1)\n",
    "#    df[list_scale] = scaler.transform(df[list_scale])\n",
    "#    return df\n",
    "#\n",
    "#dummies_scale(customer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
